# RCU基础RCU是个比较大的话题，这里分3篇介绍，原文来自RCU作者Paul E. McKenney在perfbook和LWN上的介绍。## 一、前言RCU是一种延迟处理的方式，延迟处理的思想嘛，在 *dawn of recorded history* 之前就出现了，说白了就是懒。前面我们讲了[spin lock](4-自旋锁.md)，[rw spin lock](5-rw自旋锁.md)和[seq lock](6-顺序锁.md)，为何又出现了RCU这样的同步机制呢？这个问题类似于问：有了刀枪剑戟这样的工具，为何会出现流星锤这样的兵器呢？每种兵器都有自己的适用场合，内核同步机制亦然。RCU在一定的应用场景下，解决了过去同步机制的问题，这也是它之所以存在的基石。### 性能问题我们先回忆一下传统的spin lock、RW spin lock和seq lock的基本原理。对于spin lock而言，临界区的保护是通过next和owner这两个共享变量进行的。线程调用`spin_lock`进入临界区，这里包括了三个动作：1. 获取了自己的号码牌 *（也就是`next`值）* 和允许哪一个号码牌进入临界区 *（`owner`）*2. 设定下一个进入临界区的号码牌 *（`next`++）*3. 判断自己的号码牌是否是允许进入的那个号码牌 *（`next == owner`）* ，如果是，进入临界区，否者spin *（不断的获取`owner`的值，判断是否等于自己的号码牌，对于ARM64处理器而言，可以使用WFE来降低功耗）* 。> 注意：1是取值，2是更新并写回，因此1和2必须是原子操作，中间不能插入任何的操作。这里解释的是传统spinlock，不是现在的qspinlock实现。线程调用`spin_unlock`离开临界区，执行`owner++`，表示下一个线程可以进入。RW spin lock和seq lock都类似spin lock，它们都是基于一个memory中的共享变量 *（对该变量的访问是原子的）* 。我们假设系统架构如下：![img](./7-rcu基础.assets/a5f79a99b31a8ce21c2454333653987220150422042122.gif)当线程在多个cpu上争抢进入临界区的时候，都会操作那个在多个cpu之间共享的数据lock *（玫瑰色的block）*。CPU0操作了lock，为了数据的一致性，CPU0的操作会导致其他cpu的L1中的lock变成无效，在随后的来自其他cpu对lock的访问会导致L1 cache miss *（更准确的说是communication cache miss）* ，必须从下一个level的cache中获取，同样的，其他cpu的L1 cache中的lock也被设定为invalid，从而引起下一次其他cpu上的communication cache miss。其实目前已经用qspinlock作为spinlock的默认实现，所以这个问题已经不存在了，但当年引入rcu时应该有这方面的考量的。更大的问题是锁被某个线程持有后，别的线程就要等了，这样浪费了大量的CPU时间。RCU的read side不需要访问这样的“共享数据”，从而极大的提升了reader侧的性能。### reader和writer可以并发执行spin lock是互斥的，任何时候只有一个thread *（reader or writer）* 进入临界区，rw spin lock要好一些，允许多个reader并发执行，提高了性能。不过，reader和updater不能并发执行，RCU解除了这些限制，允许一个updater *（不能多个updater进入临界区，这可以通过spinlock来保证）* 和多个reader并发执行。我们可以比较一下rw spin lock和RCU，参考下图：![rw-rcu](./7-rcu基础.assets/581bf648cf35429a18b43bab7070651a20151203045709.gif)rwlock允许多个reader并发，因此，在上图中，三个rwlock reader愉快的并行执行。当rwlock writer试图进入的时候 *（红色虚线）* ，只能spin，直到所有的reader退出临界区。一旦有rwlock writer在临界区，任何的reader都不能进入，直到writer完成数据更新，离开临界区。绿色的reader thread们又可以进行愉快玩耍了。rwlock的一个特点就是确定性，白色的reader一定是读取的是old data，而绿色的reader一定获取的是writer更新之后的new data。RCU和传统的锁机制不同，当RCU updater进入临界区的时候，即便是有reader在也无所谓，它可以长驱直入，不需要spin。同样的，即便有一个updater正在临界区里面工作，这并不能阻挡RCU reader的步伐。由此可见，RCU的并发性能要好于rwlock，特别如果考虑cpu的数目比较多的情况，那些处于spin状态的cpu在无谓的消耗，多么可惜，随着cpu的数目增加，rwlock性能不断的下降。RCU reader和updater由于可以并发执行，因此这时候的被保护的数据有两份，一份是旧的，一份是新的，对于白色的RCU reader，其读取的数据可能是旧的，也可能是新的，和数据访问的timing相关，当然，当RCU update完成更新之后，新启动的RCU reader *（绿色block）* 读取的一定是新的数据。### 适用的场景我们前面说过，每种锁都有自己的适用的场景：spin lock不区分reader和writer，对于那些读写强度不对称的是不适合的，rwlock和seq lock解决了这个问题，不过seq lock倾向writer，而RW spin lock更照顾reader。看起来一切都已经很完美了，但是，随着计算机硬件技术的发展，CPU的运算速度越来越快，相比之下，存储器件的速度发展较为滞后。在这种背景下，获取基于counter *（需要访问存储器件）* 的锁 *（例如spin lock，rwlock）* 的机制开销比较大。而且，目前的趋势是：CPU和存储器件之间的速度差别在逐渐扩大。因此，那些基于一个multi-processor之间的共享的counter的锁机制已经不能满足性能的需求，在这种情况下，RCU机制应运而生 *（当然，更准确的说RCU是一种内核同步机制，但不是一种lock，本质上它是lock-free的）* ，它克服了其他锁机制的缺点，但是，甘蔗没有两头甜，RCU的使用场景比较受限，主要适用于下面的场景：1. RCU只能保护动态分配的数据结构，并且必须是通过指针访问该数据结构2. 受RCU保护的临界区内不能sleep *（SRCU不是本文的内容）*3. 读写不对称，对writer的性能没有特别要求，但是reader性能要求极高。4. reader端对新旧数据不敏感。## 二、核心APIRCU有一堆API，这里我们只看这6个，先别看表里的作用列，后面有详细解释。- 对于reader，RCU的操作包括：   | 接口名称          | 作用                                                         || ----------------- | ------------------------------------------------------------ || `rcu_read_lock`   | 用来标识RCU read side临界区的开始                            || `rcu_dereference` | 用来获取RCU protected pointer。reader要访问RCU保护的共享数据，当然要获取RCU protected pointer，然后通过该指针进行dereference的操作 || `rcu_read_unlock` | 用来标识reader离开RCU read side临界区                        |- 对于writer，RCU的操作包括：  | 接口名称             | 作用                                                         |  | -------------------- | ------------------------------------------------------------ |  | `rcu_assign_pointer` | 该接口被writer用来进行removal的操作，在writer完成新版本数据分配和更新之后，调用这个接口可以让RCU protected pointer指向RCU protected data |  | `synchronize_rcu`    | writer端的操作可以是同步的，也就是说，完成更新操作之后，可以调用该接口函数等待所有在旧版本数据上的reader线程离开临界区，一旦从该函数返回，说明旧的共享数据没有任何引用了，可以直接进行reclaimation的操作 |  | `call_rcu`           | 当然，某些情况下 *（例如在softirq context中）* ，writer无法阻塞，这时候可以调用`call_rcu`接口函数，该函数仅仅是注册了callback就直接返回了，在适当的时机会调用callback函数，完成reclaimation的操作。这样的场景其实是分开removal和reclaimation的操作在两个不同的线程中：updater和reclaimer |reader想要进入/退出临界段，调用`rcu_read_lock`/`rcu_read_unlock`就可以，这里的读临界段是可以嵌套的。在临界段里`rcu_dereference`用来读RCU保护的指针。这里我们给writer改个名字叫updater，在RCU的文档里好像都是用updater来称呼writer的。我们来看看updater用的3个api。`synchronize_rcu`是用来等reader退出临界段的，所谓等，就是它会阻塞，所有已经进了临界段的reader调用了`rcu_read_unlock`之后才能恢复运行。当然也有不阻塞的版本`call_rcu`，它的用法比较麻烦，后面再说。之所以有等待reader这个操作，是由于我们在reader处于临界段的时候修改了它要读的那个指针，我们想让这些还没调用`rcu_read_unlock`的reader读到修改前的值。毕竟他们进了读临界段，就是在告诉我们别在这时候乱改。但rcu的思想是先瞒着这些reader，我们改我们的，他们读他们的，只要确保我们改的他们读不到。这就有这样一个问题：我们改了，reader却读不到，不是白改吗？是的，已经在临界段的reader读不到，但是要让我们改了之后才进入临界段的reader读到新的值。`synchronize_rcu`就是起这个作用的，等当前所有reader离开后，进行数据同步。这个同步的目的就是让后面的reader能看到新值。```cstruct route *gptr;int access_route(int (*f)(struct route *rp)){    int ret = -1;    struct route *rp;    rcu_read_lock();    rp = rcu_dereference(gptr);    if (rp)    	ret = f(rp);    rcu_read_unlock();    return ret;}struct route *ins_route(struct route *rp){    struct route *old_rp;    spin_lock(&route_lock);    old_rp = gptr;    rcu_assign_pointer(gptr, rp);    spin_unlock(&route_lock);    return old_rp;}int del_route(void){    struct route *old_rp;    spin_lock(&route_lock);    old_rp = gptr;    RCU_INIT_POINTER(gptr, NULL);    spin_unlock(&route_lock);    synchronize_rcu();    free(old_rp);    return !!old_rp;}```## 三、RCU的基本思路理解RCU需要参透其三大基础机制：- 插入新的数据- 删除旧的数据- reader如何游刃有余的穿梭在新数据的插入和旧数据的删除过程中，并且能够全身而退### 发布订阅APIRCU机制有一个很重要的特点就是在write的同时允许reader thread对数据进行遍历扫描。为了实现这一个关键的特点，RCU使用了publish-subscribe的机制。我们举一个例子：有一个初始化为NULL的全局指针`gp`，并让`gp`指向我们给它新分配的内存，代码如下：```cstruct foo {    int a;    int b;    int c;};struct foo *gp = NULL;...p = kmalloc(sizeof(*p), GFP_KERNEL);p->a = 1;p->b = 2;p->c = 3;gp = p;```在[原子操作](1-原子操作.md)的说明中我们知道，要保证上面最后4行的执行顺序，就要给编译器和cpu施加强制措施。因此有`rcu_assign_pointer`这个宏```c#define rcu_assign_pointer(p, v) do {			              \	compiletime_assert_atomic_type(p);				\	__smp_mb();							\	WRITE_ONCE(p, v);						\} while (0)````compiletime_assert_atomic_type`保证了指针`p`可以被原子操作（只要`p`是8/16/32/64位的就可以原子操作），下面一行`__smp_mb`在ARM64中的就是`dmb ish`，保证了`WRITE_ONCE`的时候，前面的内存访问指令已经执行完了。有意思的是`WRITE_ONCE`里包含了一个`compiletime_assert_rwonce_type`验证，它在`compiletime_assert_atomic_type`的基础上还额外允许`long long`类型。由于类型判断用的是`sizeof`而非`typeof`，所以两个compile time assert宏对ARM64来说没有什么区别，因为`long`和`long long`都是64位数，但对于ARM32就很有必要：开启了LPAE后的`p`是64位的（也就是`long long`类型），不能因此拒绝执行`WRITE_ONCE`。当然，上面的代码是终极简化示意版，内核里的实现要周全的多。继续看前面提到的那4行代码，就可以写成这样：```cp->a = 1;p->b = 2;p->c = 3;rcu_assign_pointer(gp, p);```当然，只是在updater（发布者）保证顺序是不够的，`rcu_dereference`为reader（订阅者）也要有顺序保证：```crcu_read_lock();p = rcu_dereference(gp);if (p != NULL) {    do_something_with(p->a, p->b, p->c);}rcu_read_unlock();````rcu_read_lock`和`rcu_read_unlock`定义了临界区，rcu不是真正的锁，所以不会spin，也不会block。实际上，这两句在非抢占式内核中是空的。而dereference操作也只是一个`READ_ONCE`。RCU的reader临界区可以嵌套，也可以包括非常多的代码，只要这些代码不会阻塞或者睡眠 *（有一个特别的RCU叫做SRCU，它的临界区的确是允许睡眠的）* 。尽管`rcu_assign_pointer` 和` rcu_dereference` 这两个原语理论上可以使用在任何的需要RCU保护的数据，不过，在实际中还是推荐使用更高层一些的接口。比如你的场景中受RCU保护的数据是链表，那么这对原语被隐藏在链表操作API中。我们来发布一个list：```cstruct foo {    struct list_head *list;    int a, b, c;};LIST_HEAD(head);p = kmalloc(sizeof(*p), GFP_KERNEL);p->a = 1;p->b = 2;p->c = 3;spin_lock(...);list_add_rcu(&p->list, &head);  // 发布head.next=p->listspin_unlock(...);```注意：用锁来保护含有assign pointer的调用，防止并发assign。当然这种同步机制不会组织并发的rcu reader。我们来订阅上面的发布```crcu_read_lock();list_for_each_entry_rcu(ptr, head, list) {  // 包含ptr=head.next，即订阅	do_something_with(ptr->a, ptr->b, ptr->c);}rcu_read_unlock();```既然发布和订阅的时候都没干什么正事，RCU是怎么运行的呢？答案是同步函数。当writer需要对`p`进行修改的时候要调用同步函数，这里用list的replace操作作为例子。```cq = kmalloc(sizeof(*p), GFP_KERNEL);*q = *p;list_replace_rcu(&p->list, &q->list);synchronize_rcu();  // 同步函数kfree(p);```### 原理![rcu](./7-rcu基础.assets/76efb6df52d164f2e5f49e121a61e57420151203045710.gif)RCU涉及的数据有两种，一个是指向要保护数据的指针，我们称之RCU protected pointer。另外一个是通过指针访问的共享数据，我们称之RCU protected data，当然，这个数据必须是动态分配的 。对共享数据的访问有两种，一种是writer，即对数据要进行更新，另外一种是reader。如果在有reader在临界区内进行数据访问，对于传统的，基于锁的同步机制而言，reader会阻止writer进入 *（例如spin lock和rw spin lock。seqlock不会这样，因此本质上seqlock也是lock-free的）* ，因为在有reader访问共享数据的情况下，writer直接修改data会破坏掉共享数据。怎么办呢？当然是移除了reader对共享数据的访问之后，再让writer进入了 *（writer稍显悲剧）* 。对于RCU而言，其原理是类似的，为了能够让writer进入，必须首先移除reader对共享数据的访问，怎么移除呢？创建一个新的copy是一个不错的选择。因此RCU writer的动作分成了两步：1. removal。分配writer一个new version的共享数据进行数据更新，更新完毕后将RCU protected pointer指向新版本的数据。一旦把RCU protected pointer指向的新的数据，也就意味着将其推向前台，公布与众 *（reader都是通过pointer访问数据的）* 。通过这样的操作，原来read 0、1、2对共享数据的reference被移除了 *（对于新版本的受RCU保护的数据而言）* ，它们都是在旧版本的RCU protected data上进行数据访问。2. reclamation。共享数据不能有两个版本，因此一定要在适当的时机去回收旧版本的数据。当然，不能太着急，不能reader线程还访问着old version的数据的时候就强行回收，这样会让reader crash的。reclamation必须发生在所有的访问旧版本数据的那些reader离开临界区之后再回收，而这段等待的时间被称为grace period。顺便说明一下，reclamation并不需要等待read3和4，因为write端的为RCU protected pointer赋值的语句是原子的，乱入的reader线程要么看到的是旧的数据，要么是新的数据。对于read3和4，它们访问的是新的共享数据，因此不会reference旧的数据，因此reclamation不需要等待read3和4离开临界区。### 基本RCU操作|      |      || ---- | ---- ||      |      ||      |      ||      |      |对于writer，RCU的操作包括：|      |      || ---- | ---- ||      |      ||      |      ||      |      | ## 四、参考文档- perfbook- linux-4.1.10\Documentation\RCU\*